// Voice Chat App for ã‚ªãƒ¼ãƒ«ã‚¤ãƒ³ç•ªé•·
// Configuration
const CONFIG = {
  // Use local proxy (server.js handles the API call)
  API_URL: '/api/chat',
  MODEL: 'anthropic/claude-sonnet-4-20250514'
};

class VoiceChatApp {
  constructor() {
    this.recognition = null;
    this.synthesis = window.speechSynthesis;
    this.isRecording = false;
    this.isSpeaking = false;
    this.blinkInterval = null;
    this.conversationHistory = [];
    
    // DOM Elements
    this.micButton = document.getElementById('micButton');
    this.status = document.getElementById('status');
    this.messageBox = document.getElementById('messageBox');
    this.voiceSelect = document.getElementById('voiceSelect');
    this.mouth = document.getElementById('mouth');
    this.eyeLeft = document.getElementById('eyeLeft');
    this.eyeRight = document.getElementById('eyeRight');
    
    this.init();
  }
  
  init() {
    console.log('ğŸ° Voice Chat App initializing...');
    
    // Check HTTPS
    if (location.protocol !== 'https:' && location.hostname !== 'localhost') {
      console.warn('âš ï¸ HTTPS required for microphone access');
      this.status.textContent = 'HTTPSãŒå¿…è¦ã§ã™';
    }
    
    // Initialize Speech Recognition
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      console.log('âœ… Speech Recognition supported');
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      this.recognition = new SpeechRecognition();
      this.recognition.lang = 'ja-JP';
      this.recognition.continuous = false;
      this.recognition.interimResults = true;
      
      this.recognition.onstart = () => this.onRecordingStart();
      this.recognition.onresult = (e) => this.onRecordingResult(e);
      this.recognition.onend = () => this.onRecordingEnd();
      this.recognition.onerror = (e) => this.onRecordingError(e);
    } else {
      console.error('âŒ Speech Recognition not supported');
      this.status.textContent = 'éŸ³å£°èªè­˜éå¯¾å¿œã®ãƒ–ãƒ©ã‚¦ã‚¶ã§ã™';
      this.micButton.disabled = true;
    }
    
    // Event Listeners
    this.micButton.addEventListener('click', () => {
      console.log('ğŸ¤ Mic button clicked');
      this.toggleRecording();
    });
    
    // Start blinking animation
    this.startBlinking();
    console.log('âœ… App initialized');
  }
  
  async toggleRecording() {
    if (this.isRecording) {
      console.log('â¹ï¸ Stopping recording');
      this.recognition.stop();
    } else {
      // Check microphone permission first
      try {
        console.log('ğŸ” Requesting microphone permission...');
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        stream.getTracks().forEach(track => track.stop()); // Release immediately
        console.log('âœ… Microphone permission granted');
        
        console.log('â–¶ï¸ Starting recording');
        this.recognition.start();
      } catch (err) {
        console.error('âŒ Microphone permission denied:', err);
        this.status.textContent = 'ãƒã‚¤ã‚¯ã®è¨±å¯ãŒå¿…è¦ã§ã™';
        this.status.className = 'status-indicator';
        alert('ãƒã‚¤ã‚¯ã®ä½¿ç”¨ã‚’è¨±å¯ã—ã¦ãã ã•ã„');
      }
    }
  }
  
  onRecordingStart() {
    this.isRecording = true;
    this.micButton.classList.add('recording');
    this.status.textContent = 'èã„ã¦ã„ã¾ã™...';
    this.status.className = 'status-indicator listening';
  }
  
  onRecordingResult(event) {
    let transcript = '';
    for (let i = event.resultIndex; i < event.results.length; i++) {
      transcript += event.results[i][0].transcript;
    }
    
    // Show interim results
    if (event.results[event.results.length - 1].isFinal) {
      this.addMessage(transcript, 'user');
      this.sendToOpenClaw(transcript);
    }
  }
  
  onRecordingEnd() {
    this.isRecording = false;
    this.micButton.classList.remove('recording');
    this.status.textContent = 'å¾…æ©Ÿä¸­...';
    this.status.className = 'status-indicator';
  }
  
  onRecordingError(event) {
    console.error('âŒ Speech recognition error:', event.error);
    
    const errorMessages = {
      'no-speech': 'éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ',
      'audio-capture': 'ãƒã‚¤ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
      'not-allowed': 'ãƒã‚¤ã‚¯ã®è¨±å¯ãŒå¿…è¦ã§ã™',
      'network': 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼',
      'aborted': 'ä¸­æ–­ã•ã‚Œã¾ã—ãŸ',
      'service-not-allowed': 'ã‚µãƒ¼ãƒ“ã‚¹ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“'
    };
    
    this.status.textContent = errorMessages[event.error] || `ã‚¨ãƒ©ãƒ¼: ${event.error}`;
    this.status.className = 'status-indicator';
    this.isRecording = false;
    this.micButton.classList.remove('recording');
  }
  
  addMessage(text, sender) {
    const div = document.createElement('div');
    div.className = `message ${sender}`;
    div.textContent = text;
    this.messageBox.appendChild(div);
    this.messageBox.scrollTop = this.messageBox.scrollHeight;
  }
  
  async sendToOpenClaw(message) {
    console.log('ğŸ“¤ Sending to API:', message);
    this.status.textContent = 'è€ƒãˆä¸­...';
    this.status.className = 'status-indicator thinking';
    
    // Add to conversation history
    this.conversationHistory.push({ role: 'user', content: message });
    
    try {
      // Call local proxy
      console.log('ğŸŒ Fetching:', CONFIG.API_URL);
      const response = await fetch(CONFIG.API_URL, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: CONFIG.MODEL,
          messages: this.conversationHistory.slice(-10), // Keep last 10 messages
          stream: false
        })
      });
      
      console.log('ğŸ“¥ Response status:', response.status);
      
      if (!response.ok) {
        const errorText = await response.text();
        console.error('âŒ API error response:', errorText);
        throw new Error(`HTTP ${response.status}: ${errorText}`);
      }
      
      const data = await response.json();
      console.log('ğŸ“¥ Response data:', data);
      const reply = data.choices?.[0]?.message?.content || 'ã”ã‚ã‚“ã€ã†ã¾ãè¿”ç­”ã§ããªã‹ã£ãŸ...';
      
      // Add to history
      this.conversationHistory.push({ role: 'assistant', content: reply });
      
      this.addMessage(reply, 'assistant');
      this.speak(reply);
      
    } catch (error) {
      console.error('âŒ API Error:', error);
      const fallbackReply = 'ã”ã‚ã‚“ã­ã€æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒèµ·ããŸã¿ãŸã„ã€‚ã‚‚ã†ä¸€åº¦è©¦ã—ã¦ã­ï¼';
      this.addMessage(fallbackReply, 'assistant');
      this.speak(fallbackReply);
    }
  }
  
  speak(text) {
    // Cancel any ongoing speech
    this.synthesis.cancel();
    
    const voiceType = this.voiceSelect.value;
    
    if (voiceType === 'zundamon') {
      // Try VOICEVOX for Zundamon (if available)
      this.speakWithVoicevox(text);
    } else {
      this.speakWithWebSpeech(text);
    }
  }
  
  speakWithWebSpeech(text) {
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = 'ja-JP';
    utterance.rate = 1.1;
    utterance.pitch = 1.2;
    
    // Find Japanese voice
    const voices = this.synthesis.getVoices();
    const japaneseVoice = voices.find(v => v.lang.includes('ja'));
    if (japaneseVoice) {
      utterance.voice = japaneseVoice;
    }
    
    utterance.onstart = () => this.onSpeakStart();
    utterance.onend = () => this.onSpeakEnd();
    
    this.synthesis.speak(utterance);
  }
  
  async speakWithVoicevox(text) {
    try {
      // VOICEVOX API (ãšã‚“ã ã‚‚ã‚“ = speaker 3, æ—©å£)
      const queryRes = await fetch(`http://localhost:50021/audio_query?text=${encodeURIComponent(text)}&speaker=3`, {
        method: 'POST'
      });
      
      if (!queryRes.ok) throw new Error('VOICEVOX query failed');
      
      const query = await queryRes.json();
      query.speedScale = 1.5; // æ—©å£
      
      const audioRes = await fetch('http://localhost:50021/synthesis?speaker=3', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(query)
      });
      
      if (!audioRes.ok) throw new Error('VOICEVOX synthesis failed');
      
      const audioBlob = await audioRes.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);
      
      audio.onplay = () => this.onSpeakStart();
      audio.onended = () => {
        this.onSpeakEnd();
        URL.revokeObjectURL(audioUrl);
      };
      
      audio.play();
      
    } catch (error) {
      console.warn('VOICEVOX unavailable, falling back to Web Speech:', error);
      this.speakWithWebSpeech(text);
    }
  }
  
  onSpeakStart() {
    this.isSpeaking = true;
    this.status.textContent = 'è©±ã—ã¦ã„ã¾ã™...';
    this.status.className = 'status-indicator speaking';
    this.mouth.classList.add('talking');
  }
  
  onSpeakEnd() {
    this.isSpeaking = false;
    this.status.textContent = 'å¾…æ©Ÿä¸­...';
    this.status.className = 'status-indicator';
    this.mouth.classList.remove('talking');
  }
  
  startBlinking() {
    // Random blinking every 2-5 seconds
    const blink = () => {
      this.eyeLeft.classList.add('blinking');
      this.eyeRight.classList.add('blinking');
      
      setTimeout(() => {
        this.eyeLeft.classList.remove('blinking');
        this.eyeRight.classList.remove('blinking');
      }, 150);
      
      // Schedule next blink
      const nextBlink = 2000 + Math.random() * 3000;
      setTimeout(blink, nextBlink);
    };
    
    // Start first blink after 1 second
    setTimeout(blink, 1000);
  }
}

// Initialize when DOM is ready
document.addEventListener('DOMContentLoaded', () => {
  // Wait for voices to load
  if (speechSynthesis.onvoiceschanged !== undefined) {
    speechSynthesis.onvoiceschanged = () => {
      new VoiceChatApp();
    };
  } else {
    new VoiceChatApp();
  }
});
